# MixedLRMoE.jl

**MixedLRMoE.jl** is an implementation of the Mixed Logit-Reduced Mixture-of-Experts (Mixed LRMoE) model in `julia`.
This theoretical development of the Mixed LRMoE is given in [Fung and Tseung (2022+)](https://arxiv.org/abs/2209.15207), while an application in the automobile insurance context is given in [Tseung et al. (2023)](https://arxiv.org/abs/2209.15212).

To install the stable version of the package, simply type the following in the `julia` REPL:
```julia
] add MixedLRMoE
```

To install the latest version, type the following in the `julia` REPL:
```julia
] add https://github.com/sparktseung/MixedLRMoE.jl
```

The website of full documentation is [here](https://work.sparktseung.com/MixedLRMoE.jl/dev/).
